{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/collected_attributes2.csv')\n",
    "selected = pd.read_csv('../../data/SELECTED_LABELLED_DATASET.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = pd.read_csv('../../data/INITIAL_DATASET.csv')\n",
    "labelled = pd.read_csv('../../data/LABELLED_DATASET.csv')\n",
    "collected_attributes1 = pd.read_csv('../../data/collected_attributes1.csv')\n",
    "collected_attributes2 = pd.read_csv('../../data/collected_attributes2.csv')\n",
    "self = pd.read_csv('../../data/authors_self_conflicts.csv')\n",
    "dataset = pd.read_csv('../../data/dataset.csv')\n",
    "selected = pd.read_csv('../../data/selected_dataset.csv')\n",
    "\n",
    "dataset_training = pd.read_csv('../../data/dataset-training.csv')\n",
    "dataset_test = pd.read_csv('../../data/dataset-test.csv')\n",
    "complete = pd.concat([dataset_training, dataset_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL_DATASET: 175,805 chunks, 25,328 merges, 2,731 projects.\n",
      "LABELLED_DATASET: 175,805 chunks, 25,328 merges, 2,731 projects.\n",
      "collected_attributes1: 24,656 merges, 2,597 projects.\n",
      "collected_attributes2: 165,009 chunks, 24,222 merges, 2,566 projects.\n",
      "self_conflicts: 129,438 chunks\n",
      "dataset: 175,805 chunks, 25,328 merges, 2,731 projects.\n",
      "selected: 57,214 chunks, 4,644 merges, 23 projects.\n",
      "dataset training: 45,781 chunks, 4,222 merges, 23 projects.\n",
      "dataset test: 11,433 chunks, 2,099 merges, 23 projects.\n",
      "Invalid (not cloned) projects: 134\n",
      "Total number of chunks from not cloned projects: 3,363\n"
     ]
    }
   ],
   "source": [
    "selected_chunks = list(selected['chunk_id'].unique())\n",
    "collected_attributes2_selected = collected_attributes2[collected_attributes2['chunk_id'].isin(selected_chunks)]\n",
    "collected_attributes2_selected = collected_attributes2[collected_attributes2['chunk_id'].isin(selected_chunks)]\n",
    "\n",
    "\n",
    "print(f\"INITIAL_DATASET: {len(initial['chunk_id'].unique()):,} chunks, {len(initial['sha'].unique()):,} merges, {len(initial['project'].unique()):,} projects.\")\n",
    "print(f\"LABELLED_DATASET: {len(labelled['chunk_id'].unique()):,} chunks, {len(labelled['sha'].unique()):,} merges, {len(labelled['project'].unique()):,} projects.\")\n",
    "print(f\"collected_attributes1: {len(collected_attributes1['sha'].unique()):,} merges, {len(collected_attributes1['project'].unique()):,} projects.\")\n",
    "print(f\"collected_attributes2: {len(collected_attributes2['chunk_id'].unique()):,} chunks, {len(collected_attributes2['sha'].unique()):,} merges, {len(collected_attributes2['project'].unique()):,} projects.\")\n",
    "print(f\"self_conflicts: {len(self['chunk_id'].unique()):,} chunks\")\n",
    "print(f\"dataset: {len(dataset['chunk_id'].unique()):,} chunks, {len(dataset['sha'].unique()):,} merges, {len(dataset['project'].unique()):,} projects.\")\n",
    "print(f\"selected: {len(selected['chunk_id'].unique()):,} chunks, {len(selected['sha'].unique()):,} merges, {len(selected['project'].unique()):,} projects.\")\n",
    "\n",
    "print(f\"dataset training: {len(dataset_training['chunk_id'].unique()):,} chunks, {len(dataset_training['sha'].unique()):,} merges, {len(dataset_training['project'].unique()):,} projects.\")\n",
    "print(f\"dataset test: {len(dataset_test['chunk_id'].unique()):,} chunks, {len(dataset_test['sha'].unique()):,} merges, {len(dataset_test['project'].unique()):,} projects.\")\n",
    "\n",
    "# print(f\"selected_collected_attributes2: {len(collected_attributes2_selected['chunk_id'].unique())} chunks, {len(collected_attributes2_selected['sha'].unique())} merges, {len(collected_attributes2_selected['project'].unique())} projects.\")\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"LABELLED_DATASET: {len(pd.read_csv('../../data/LABELLED_DATASET.csv'))}\")\n",
    "# print(f\"collected_attributes1.csv: {len(pd.read_csv('../../data/collected_attributes1.csv'))}\")\n",
    "\n",
    "\n",
    "# quantos projetos estão em initial_dataset mas não em collected_attributes1?\n",
    "initial_projects = set(initial['project'].unique())\n",
    "collected_attributes1_projects = set(collected_attributes1['project'].unique())\n",
    "not_cloned = initial_projects - collected_attributes1_projects\n",
    "print(f\"Invalid (not cloned) projects: {len(not_cloned):,}\")\n",
    "\n",
    "# quantos chunks têm os invalid projects?\n",
    "chunks_not_cloned = initial[initial['project'].isin(not_cloned)]\n",
    "print(f\"Total number of chunks from not cloned projects: {len(chunks_not_cloned):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks in the selected dataset with at least one missing value: 14138 from 23 projects\n"
     ]
    }
   ],
   "source": [
    "null_data = selected[selected.isnull().any(axis=1)]\n",
    "print(f\"Chunks in the selected dataset with at least one missing value: {len(null_data)} from {len(null_data['project'].unique())} projects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks in the selected dataset without missing values: 43076 from 21 projects\n"
     ]
    }
   ],
   "source": [
    "selected_non_na = selected.dropna()\n",
    "print(f\"Chunks in the selected dataset without missing values: {len(selected_non_na)} from {len(selected_non_na['project'].unique())} projects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing the 3 projects without enough data for the classification\n",
      "final dataset training: 42,007 chunks, 3,675 merges, 20 projects.\n",
      "final dataset test: 10,491 chunks, 1,821 merges, 20 projects.\n"
     ]
    }
   ],
   "source": [
    "# Dropped projects due to missing values: elasticsearch, project, open\n",
    "dropped_projects = ['elasticsearch', 'jetty.project', 'com.revolsys.open']\n",
    "training_without_missing = dataset_training[~dataset_training['project_name'].isin(dropped_projects)]\n",
    "complete_without_missing = complete[~complete['project_name'].isin(dropped_projects)]\n",
    "test_without_missing = dataset_test[~dataset_test['project_name'].isin(dropped_projects)]\n",
    "print('Removing the 3 projects without enough data for the classification')\n",
    "print(f\"final dataset training: {len(training_without_missing['chunk_id'].unique()):,} chunks, {len(training_without_missing['sha'].unique()):,} merges, {len(training_without_missing['project'].unique()):,} projects.\")\n",
    "print(f\"final dataset test: {len(test_without_missing['chunk_id'].unique()):,} chunks, {len(test_without_missing['sha'].unique()):,} merges, {len(test_without_missing['project'].unique()):,} projects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
